{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ZIAkIlfmCe1B"
      },
      "cell_type": "markdown",
      "source": [
        "# The Hello World of Deep Learning with Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "fA93WUy1zzWf"
      },
      "cell_type": "markdown",
      "source": [
        "Like every first app you should start with something super simple that shows the overall scaffolding for how your code works. \n",
        "\n",
        "In the case of creating neural networks, the sample I like to use is one where it learns the relationship between two numbers. So, for example, if you were writing code for a function like this, you already know the 'rules' -- \n",
        "\n",
        "\n",
        "```\n",
        "float my_function(float x){\n",
        "    float y = (3 * x) + 1;\n",
        "    return y;\n",
        "}\n",
        "```\n",
        "\n",
        "So how would you train a neural network to do the equivalent task? Using data! By feeding it with a set of Xs, and a set of Ys, it should be able to figure out the relationship between them. \n",
        "\n",
        "This is obviously a very different paradigm than what you might be used to, so let's step through it piece by piece.\n"
      ]
    },
    {
      "metadata": {
        "id": "DzbtdRcZDO9B"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Let's start with our imports. Here we are importing TensorFlow and calling it tf for ease of use.\n",
        "\n",
        "We then import a library called numpy, which helps us to represent our data as lists easily and quickly.\n",
        "\n",
        "The framework for defining a neural network as a set of Sequential layers is called keras, so we import that too."
      ]
    },
    {
      "metadata": {
        "id": "X9uIpOS2zx7k"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwJGmDrQ0EoB"
      },
      "cell_type": "markdown",
      "source": [
        "## Define and Compile the Neural Network\n",
        "\n",
        "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value."
      ]
    },
    {
      "metadata": {
        "id": "kQFAr_xo0M4T"
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KhjZjZ-c0Ok9"
      },
      "cell_type": "markdown",
      "source": [
        "Now we compile our Neural Network. When we do so, we have to specify 2 functions, a loss and an optimizer.\n",
        "\n",
        "If you've seen lots of math for machine learning, here's where it's usually used, but in this case it's nicely encapsulated in functions for you. But what happens here -- let's explain...\n",
        "\n",
        "We know that in our function, the relationship between the numbers is y=3x+1. \n",
        "\n",
        "When the computer is trying to 'learn' that, it makes a guess...maybe y=10x+10. The LOSS function measures the guessed answers against the known correct answers and measures how well or how badly it did.\n",
        "\n",
        "It then uses the OPTIMIZER function to make another guess. Based on how the loss function went, it will try to minimize the loss. At that point maybe it will come up with somehting like y=5x+5, which, while still pretty bad, is closer to the correct result (i.e. the loss is lower)\n",
        "\n",
        "It will repeat this for the number of EPOCHS which you will see shortly. But first, here's how we tell it to use 'MEAN SQUARED ERROR' for the loss and 'STOCHASTIC GRADIENT DESCENT' for the optimizer. You don't need to understand the math for these yet, but you can see that they work! :)\n",
        "\n",
        "Over time you will learn the different and appropriate loss and optimizer functions for different scenarios. \n"
      ]
    },
    {
      "metadata": {
        "id": "m8YQN1H41L-Y"
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd', loss='mean_squared_error')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QyOUhFw1OUX"
      },
      "cell_type": "markdown",
      "source": [
        "## Providing the Data\n",
        "\n",
        "Next up we'll feed in some data. In this case we are taking 6 xs and 6ys. You can see that the relationship between these is that y=2x-1, so where x = -1, y=-3 etc. etc. \n",
        "\n",
        "A python library called 'Numpy' provides lots of array type data structures that are a defacto standard way of doing it. We declare that we want to use these by specifying the values asn an np.array[]"
      ]
    },
    {
      "metadata": {
        "id": "4Dxk4q-jzEy4"
      },
      "cell_type": "code",
      "source": [
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n_YcWRElnM_b"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "c-Jk4dG91dvD"
      },
      "cell_type": "markdown",
      "source": [
        "The process of training the neural network, where it 'learns' the relationship between the Xs and Ys is in the **model.fit**  call. This is where it will go through the loop we spoke about above, making a guess, measuring how good or bad it is (aka the loss), using the optimizer to make another guess etc. It will do it for the number of epochs you specify. When you run this code, you'll see the loss on the right hand side."
      ]
    },
    {
      "metadata": {
        "id": "lpRrl7WK10Pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac4bb8b-aa12-416b-e62d-7cdea8de64f7"
      },
      "cell_type": "code",
      "source": [
        "model.fit(xs, ys, epochs=500)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 62.0366\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 48.8075\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 38.3994\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 30.2108\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 23.7684\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 18.6999\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 14.7122\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.5749\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.1066\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1646\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.6368\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4348\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.4891\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7451\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1597\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6992\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3369\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0518\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8275\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6511\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5123\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4030\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3171\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2495\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1963\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1545\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1216\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0957\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0753\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0592\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0466\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0367\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0289\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0227\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0179\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0141\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0111\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0088\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0069\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0054\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0043\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0034\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0027\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0021\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0013\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5204e-04\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8252e-04\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.4891e-04\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.4351e-04\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6038e-04\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9473e-04\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4285e-04\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0181e-04\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6930e-04\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4350e-04\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2299e-04\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0665e-04\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.3600e-05\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.3127e-05\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.4690e-05\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.7859e-05\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.2304e-05\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.7745e-05\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.3981e-05\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.0843e-05\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8205e-05\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.5958e-05\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4026e-05\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2344e-05\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.0864e-05\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.9545e-05\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8354e-05\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.7269e-05\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6270e-05\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.5339e-05\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.4470e-05\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.3646e-05\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.2866e-05\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.2121e-05\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.1407e-05\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.0717e-05\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0052e-05\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9408e-05\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8783e-05\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8175e-05\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7583e-05\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7007e-05\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6444e-05\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5894e-05\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5358e-05\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4833e-05\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4319e-05\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3817e-05\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3327e-05\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2846e-05\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2376e-05\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1915e-05\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1464e-05\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1022e-05\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0590e-05\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0167e-05\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9753e-05\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9346e-05\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8948e-05\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8559e-05\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8178e-05\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7804e-05\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7439e-05\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7081e-05\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6730e-05\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6386e-05\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6050e-05\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5720e-05\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5397e-05\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5081e-05\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4772e-05\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4468e-05\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4170e-05\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3879e-05\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3594e-05\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3314e-05\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3041e-05\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2773e-05\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2510e-05\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2254e-05\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2002e-05\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1756e-05\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1514e-05\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1278e-05\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1047e-05\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0819e-05\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0597e-05\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0380e-05\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0167e-05\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.9583e-06\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.7536e-06\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.5530e-06\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3572e-06\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.1649e-06\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.9765e-06\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.7923e-06\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.6117e-06\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.4349e-06\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2619e-06\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.0919e-06\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9260e-06\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7629e-06\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6031e-06\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4471e-06\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.2943e-06\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1439e-06\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9975e-06\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8540e-06\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7129e-06\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.5750e-06\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.4403e-06\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3080e-06\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.1786e-06\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.0519e-06\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.9273e-06\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.8059e-06\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.6866e-06\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.5697e-06\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.4552e-06\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.3429e-06\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.2332e-06\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1255e-06\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0204e-06\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.9172e-06\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.8164e-06\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7174e-06\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6207e-06\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5255e-06\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4326e-06\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3418e-06\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2524e-06\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1648e-06\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0796e-06\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.9957e-06\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.9135e-06\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8329e-06\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7544e-06\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6770e-06\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6018e-06\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5278e-06\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4552e-06\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3845e-06\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3149e-06\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.2468e-06\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1801e-06\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.1147e-06\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0507e-06\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9882e-06\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9271e-06\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8669e-06\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8080e-06\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7503e-06\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6939e-06\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6388e-06\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5847e-06\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5317e-06\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4797e-06\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4286e-06\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3785e-06\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3297e-06\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2820e-06\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.2351e-06\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1890e-06\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1441e-06\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0999e-06\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0568e-06\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0146e-06\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9731e-06\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9325e-06\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8928e-06\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8538e-06\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8158e-06\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7786e-06\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7420e-06\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7063e-06\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6712e-06\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6370e-06\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6031e-06\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5703e-06\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5381e-06\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5066e-06\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4756e-06\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4453e-06\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4154e-06\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3863e-06\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3578e-06\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3300e-06\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3029e-06\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2761e-06\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2498e-06\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2240e-06\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1989e-06\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1743e-06\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1502e-06\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1267e-06\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1033e-06\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0808e-06\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0587e-06\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0371e-06\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0156e-06\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.9458e-07\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7431e-07\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.5427e-07\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.3468e-07\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.1542e-07\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.9667e-07\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.7807e-07\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5998e-07\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.4239e-07\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.2523e-07\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.0816e-07\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.9163e-07\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.7535e-07\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.5954e-07\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.4390e-07\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2853e-07\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1369e-07\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.9896e-07\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8464e-07\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.7062e-07\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.5675e-07\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.4328e-07\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.3007e-07\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.1706e-07\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.0436e-07\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9198e-07\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.7987e-07\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.6804e-07\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.5646e-07\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.4497e-07\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.3364e-07\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.2269e-07\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1201e-07\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.0148e-07\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.9126e-07\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8118e-07\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7137e-07\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6148e-07\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.5201e-07\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.4283e-07\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.3371e-07\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2485e-07\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.1604e-07\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0750e-07\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.9915e-07\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.9090e-07\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8289e-07\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.7500e-07\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6739e-07\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.5987e-07\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5250e-07\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4527e-07\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3819e-07\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3129e-07\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.2443e-07\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.1775e-07\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.1124e-07\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.0487e-07\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9863e-07\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9245e-07\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.8652e-07\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8059e-07\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7480e-07\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6912e-07\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6362e-07\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5824e-07\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5294e-07\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4780e-07\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4271e-07\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3764e-07\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3279e-07\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2798e-07\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2332e-07\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1868e-07\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1417e-07\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0974e-07\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0541e-07\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0123e-07\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9720e-07\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9316e-07\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8917e-07\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8532e-07\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8155e-07\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7773e-07\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7410e-07\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7050e-07\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6708e-07\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6362e-07\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6031e-07\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5705e-07\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5389e-07\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5072e-07\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4761e-07\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4464e-07\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4160e-07\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3876e-07\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3588e-07\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3308e-07\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3037e-07\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2769e-07\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2501e-07\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2240e-07\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1995e-07\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1744e-07\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1509e-07\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1275e-07\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1044e-07\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0817e-07\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0598e-07\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0381e-07\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0164e-07\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9622e-08\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7538e-08\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.5560e-08\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.3580e-08\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.1668e-08\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.9761e-08\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.7905e-08\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6097e-08\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.4358e-08\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2622e-08\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.0966e-08\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.9283e-08\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7695e-08\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6084e-08\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.4490e-08\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.2984e-08\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1511e-08\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0035e-08\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.8578e-08\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.7181e-08\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.5783e-08\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.4449e-08\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3101e-08\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.1812e-08\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.0520e-08\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9271e-08\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.8102e-08\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.6868e-08\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.5712e-08\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.4587e-08\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.3473e-08\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.2387e-08\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1327e-08\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0288e-08\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9251e-08\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8235e-08\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7262e-08\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.6282e-08\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.5328e-08\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.4410e-08\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.3502e-08\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.2603e-08\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.1713e-08\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0857e-08\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0025e-08\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.9187e-08\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8386e-08\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7604e-08\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6822e-08\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6088e-08\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5348e-08\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.4598e-08\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3889e-08\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.3223e-08\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.2552e-08\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1862e-08\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.1252e-08\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0600e-08\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0000e-08\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.9361e-08\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8773e-08\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8171e-08\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7596e-08\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7058e-08\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6497e-08\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5960e-08\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5404e-08\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4912e-08\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4380e-08\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3888e-08\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3374e-08\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2920e-08\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2451e-08\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1986e-08\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1546e-08\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1080e-08\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0629e-08\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0238e-08\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9797e-08\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9394e-08\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8986e-08\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8596e-08\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8215e-08\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7854e-08\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7506e-08\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7169e-08\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6813e-08\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6466e-08\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6131e-08\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5801e-08\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5493e-08\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5175e-08\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4859e-08\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4559e-08\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4277e-08\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3972e-08\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3710e-08\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3419e-08\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3154e-08\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2887e-08\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2634e-08\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2376e-08\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2101e-08\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1871e-08\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1629e-08\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1403e-08\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1166e-08\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0945e-08\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0713e-08\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0496e-08\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0269e-08\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0083e-08\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.8800e-09\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.6833e-09\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.4776e-09\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.2816e-09\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.1042e-09\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.9042e-09\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.7075e-09\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.5367e-09\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.3500e-09\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.1697e-09\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.0219e-09\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.8467e-09\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6913e-09\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.5344e-09\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.3936e-09\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2251e-09\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0773e-09\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9255e-09\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7920e-09\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.6511e-09\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5073e-09\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3789e-09\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.2630e-09\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1303e-09\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.0032e-09\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.8877e-09\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.7628e-09\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.6421e-09\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.5172e-09\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.4179e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc89830e020>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "kaFIr71H2OZ-"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now you have a model that has been trained to learn the relationshop between X and Y. You can use the **model.predict** method to have it figure out the Y for a previously unknown X. So, for example, if X = 10, what do you think Y will be? Take a guess before you run this code:"
      ]
    },
    {
      "metadata": {
        "id": "oxNzL4lS2Gui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd691bc-97a6-4f3e-caa6-90484c086718"
      },
      "cell_type": "code",
      "source": [
        "print(model.predict([10.0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 117ms/step\n",
            "[[30.999784]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "btF2CSFH2iEX"
      },
      "cell_type": "markdown",
      "source": [
        "You might have thought 31, right? But it ended up being a little over. Why do you think that is? \n",
        "\n",
        "Remember that neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between X and Y is Y=3X+1, but with only 6 data points we can't know for sure. As a result, the result for 10 is very close to 31, but not necessarily 31. \n",
        "\n",
        "As you work with neural networks, you'll see this pattern recurring. You will almost always deal with probabilities, not certainties, and will do a little bit of coding to figure out what the result is based on the probabilities, particularly when it comes to classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2"
      ],
      "metadata": {
        "id": "an04VUA7Z8oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XIo3y3JaBuu",
        "outputId": "6da8cd82-1e48-4798-eb2b-844fcbdb6936"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenarás una red neuronal para que reconozca artículos de ropa de un conjunto de datos común llamado Fashion MNIST. Contiene 70,000 prendas de ropa en 10 categorías diferentes. Cada ropa está en una imagen en escala de grises de 28 × 28. Las etiquetas asociadas con el conjunto de datos son las siguientes:\n",
        "\n",
        "Etiqueta\n",
        "\n",
        "Descripción\n",
        "\n",
        "0\n",
        "\n",
        "Camiseta/top\n",
        "\n",
        "1\n",
        "\n",
        "Pantalón\n",
        "\n",
        "2\n",
        "\n",
        "Suéter\n",
        "\n",
        "3\n",
        "\n",
        "Vestido\n",
        "\n",
        "4\n",
        "\n",
        "Abrigo\n",
        "\n",
        "5\n",
        "\n",
        "Sandalias\n",
        "\n",
        "6\n",
        "\n",
        "Camisa\n",
        "\n",
        "7\n",
        "\n",
        "Zapatos deportivos\n",
        "\n",
        "8\n",
        "\n",
        "Bag\n",
        "\n",
        "9\n",
        "\n",
        "Botas al tobillo\n",
        "\n",
        "Los datos de Fashion MNIST están disponibles en la API de tf.keras.datasets. Cárgalo de la siguiente manera:"
      ],
      "metadata": {
        "id": "R7noVnNdaMaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "metadata": {
        "id": "cGoiOsNsaNhx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SdTJVAtxaiMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSjpHvV0aicb",
        "outputId": "67e65826-b8ef-4a90-c538-4ecb0da40622"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[1])\n",
        "print(training_labels[1])\n",
        "print(training_images[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IbB30Sn-aniP",
        "outputId": "6f1d0db0-71ff-44e8-b1fa-2c97138329ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[[0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.         0.         0.16078431 0.7372549\n",
            "  0.40392157 0.21176471 0.18823529 0.16862745 0.34117647 0.65882353\n",
            "  0.52156863 0.0627451  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.00392157 0.         0.\n",
            "  0.         0.19215686 0.53333333 0.85882353 0.84705882 0.89411765\n",
            "  0.9254902  1.         1.         1.         1.         0.85098039\n",
            "  0.84313725 0.99607843 0.90588235 0.62745098 0.17647059 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.05490196\n",
            "  0.69019608 0.87058824 0.87843137 0.83137255 0.79607843 0.77647059\n",
            "  0.76862745 0.78431373 0.84313725 0.8        0.79215686 0.78823529\n",
            "  0.78823529 0.78823529 0.81960784 0.85490196 0.87843137 0.64313725\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.7372549\n",
            "  0.85882353 0.78431373 0.77647059 0.79215686 0.77647059 0.78039216\n",
            "  0.78039216 0.78823529 0.76862745 0.77647059 0.77647059 0.78431373\n",
            "  0.78431373 0.78431373 0.78431373 0.78823529 0.78431373 0.88235294\n",
            "  0.16078431 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.2        0.85882353\n",
            "  0.78039216 0.79607843 0.79607843 0.83137255 0.93333333 0.97254902\n",
            "  0.98039216 0.96078431 0.97647059 0.96470588 0.96862745 0.98823529\n",
            "  0.97254902 0.92156863 0.81176471 0.79607843 0.79607843 0.87058824\n",
            "  0.54901961 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.45490196 0.88627451\n",
            "  0.80784314 0.8        0.81176471 0.8        0.39607843 0.29411765\n",
            "  0.18431373 0.28627451 0.18823529 0.19607843 0.17647059 0.2\n",
            "  0.24705882 0.44313725 0.87058824 0.79215686 0.80784314 0.8627451\n",
            "  0.87843137 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.78431373 0.87058824\n",
            "  0.81960784 0.79607843 0.84313725 0.78431373 0.         0.2745098\n",
            "  0.38431373 0.         0.40392157 0.23137255 0.26666667 0.27843137\n",
            "  0.19215686 0.         0.85882353 0.80784314 0.83921569 0.82352941\n",
            "  0.98039216 0.14901961 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.96862745 0.85490196\n",
            "  0.83137255 0.82352941 0.84313725 0.83921569 0.         0.99607843\n",
            "  0.95294118 0.54509804 1.         0.68235294 0.98431373 1.\n",
            "  0.80392157 0.         0.84313725 0.85098039 0.83921569 0.81568627\n",
            "  0.8627451  0.37254902 0.         0.        ]\n",
            " [0.         0.         0.         0.17647059 0.88627451 0.83921569\n",
            "  0.83921569 0.84313725 0.87843137 0.80392157 0.         0.16470588\n",
            "  0.1372549  0.23529412 0.0627451  0.06666667 0.04705882 0.05098039\n",
            "  0.2745098  0.         0.74117647 0.84705882 0.83137255 0.80784314\n",
            "  0.83137255 0.61176471 0.         0.        ]\n",
            " [0.         0.         0.         0.64313725 0.92156863 0.83921569\n",
            "  0.82745098 0.8627451  0.84705882 0.78823529 0.20392157 0.27843137\n",
            "  0.34901961 0.36862745 0.3254902  0.30588235 0.2745098  0.29803922\n",
            "  0.36078431 0.34117647 0.80784314 0.81176471 0.87058824 0.83529412\n",
            "  0.85882353 0.81568627 0.         0.        ]\n",
            " [0.         0.         0.         0.41568627 0.73333333 0.8745098\n",
            "  0.92941176 0.97254902 0.82745098 0.77647059 0.98823529 0.98039216\n",
            "  0.97254902 0.96078431 0.97254902 0.98823529 0.99215686 0.98039216\n",
            "  0.98823529 0.9372549  0.78823529 0.83137255 0.88235294 0.84313725\n",
            "  0.75686275 0.44313725 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.06666667\n",
            "  0.21176471 0.62352941 0.87058824 0.75686275 0.81568627 0.75294118\n",
            "  0.77254902 0.78431373 0.78431373 0.78431373 0.78431373 0.78823529\n",
            "  0.79607843 0.76470588 0.82352941 0.64705882 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.18431373 0.88235294 0.75294118 0.83921569 0.79607843\n",
            "  0.80784314 0.8        0.8        0.80392157 0.80784314 0.8\n",
            "  0.83137255 0.77254902 0.85490196 0.41960784 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00392157 0.02352941\n",
            "  0.         0.18039216 0.83137255 0.76470588 0.83137255 0.79215686\n",
            "  0.80784314 0.80392157 0.8        0.80392157 0.80784314 0.8\n",
            "  0.83137255 0.78431373 0.85490196 0.35686275 0.         0.01176471\n",
            "  0.00392157 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.04313725 0.77254902 0.78039216 0.80392157 0.79215686\n",
            "  0.80392157 0.80784314 0.8        0.80392157 0.81176471 0.8\n",
            "  0.80392157 0.80392157 0.85490196 0.30196078 0.         0.01960784\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.01176471\n",
            "  0.         0.00784314 0.74901961 0.77647059 0.78823529 0.80392157\n",
            "  0.80784314 0.80392157 0.80392157 0.80784314 0.81960784 0.80784314\n",
            "  0.78039216 0.81960784 0.85882353 0.29019608 0.         0.01960784\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00784314\n",
            "  0.         0.         0.7372549  0.77254902 0.78431373 0.81176471\n",
            "  0.81176471 0.8        0.81176471 0.81176471 0.82352941 0.81568627\n",
            "  0.77647059 0.81176471 0.86666667 0.28235294 0.         0.01568627\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00784314\n",
            "  0.         0.         0.84313725 0.77647059 0.79607843 0.80784314\n",
            "  0.81568627 0.80392157 0.81176471 0.81176471 0.82352941 0.81568627\n",
            "  0.78431373 0.79215686 0.87058824 0.29411765 0.         0.01568627\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.83137255 0.77647059 0.81960784 0.80784314\n",
            "  0.81960784 0.80784314 0.81568627 0.81176471 0.82745098 0.80784314\n",
            "  0.80392157 0.77647059 0.86666667 0.31372549 0.         0.01176471\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.8        0.78823529 0.80392157 0.81568627\n",
            "  0.81176471 0.80392157 0.82745098 0.80392157 0.82352941 0.82352941\n",
            "  0.81960784 0.76470588 0.86666667 0.37647059 0.         0.01176471\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.79215686 0.78823529 0.80392157 0.81960784\n",
            "  0.81176471 0.80392157 0.83529412 0.80784314 0.82352941 0.81960784\n",
            "  0.82352941 0.76078431 0.85098039 0.41176471 0.         0.00784314\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.8        0.8        0.80392157 0.81568627\n",
            "  0.81176471 0.80392157 0.84313725 0.81176471 0.82352941 0.81568627\n",
            "  0.82745098 0.75686275 0.83529412 0.45098039 0.         0.00784314\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.8        0.81176471 0.81176471 0.81568627\n",
            "  0.80784314 0.80784314 0.84313725 0.82352941 0.82352941 0.81176471\n",
            "  0.83137255 0.76470588 0.82352941 0.4627451  0.         0.00784314\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.77647059 0.81568627 0.81568627 0.81568627\n",
            "  0.8        0.81176471 0.83137255 0.83137255 0.82352941 0.81176471\n",
            "  0.82745098 0.76862745 0.81176471 0.4745098  0.         0.00392157\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.77647059 0.82352941 0.81176471 0.81568627\n",
            "  0.80784314 0.81960784 0.83529412 0.83137255 0.82745098 0.81176471\n",
            "  0.82352941 0.77254902 0.81176471 0.48627451 0.         0.00392157\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.6745098  0.82352941 0.79607843 0.78823529\n",
            "  0.78039216 0.8        0.81176471 0.80392157 0.8        0.78823529\n",
            "  0.80392157 0.77254902 0.80784314 0.49803922 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.7372549  0.86666667 0.83921569 0.91764706\n",
            "  0.9254902  0.93333333 0.95686275 0.95686275 0.95686275 0.94117647\n",
            "  0.95294118 0.83921569 0.87843137 0.63529412 0.         0.00784314\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.         0.54509804 0.57254902 0.50980392 0.52941176\n",
            "  0.52941176 0.5372549  0.49019608 0.48627451 0.49019608 0.4745098\n",
            "  0.46666667 0.44705882 0.50980392 0.29803922 0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "9T2BBADaatwF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential** define una secuencia de capas en la red neuronal.\n",
        "\n",
        "**Flatten** toma un cuadrado y lo convierte en un vector unidimensional.\n",
        "\n",
        "**Dense** agrega una capa de neuronas.\n",
        "\n",
        "Las funciones **Activation** indican a cada capa de neuronas qué hacer. \n",
        "Hay muchas opciones, pero úsalas por ahora:\n",
        "\n",
        "**Relu** significa que, si X es mayor que 0, se muestra X; de lo contrario, se muestra 0. Solo pasa valores de 0 o mayores a la siguiente capa de la red.\n",
        "\n",
        "**Softmax** toma un conjunto de valores y elige el más alto con eficacia. Por ejemplo, si el resultado de la última capa es [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], evita que debas ordenar por el valor más grande; muestra [0,0,0,0,1,0,0,0,0."
      ],
      "metadata": {
        "id": "gn_3eZ_CbX66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu), \n",
        "                                    #tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "metadata": {
        "id": "w08MBr8QbTMv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB6c4qs-cQ0R",
        "outputId": "700fa211-7f83-49ce-ca8c-2e0357c6819e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.8787 - accuracy: 0.7031\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.5502 - accuracy: 0.8047\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.4907 - accuracy: 0.8256\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.4578 - accuracy: 0.8379\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.4360 - accuracy: 0.8457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc88a19e170>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7U0l4j0dGvO",
        "outputId": "b1db9227-ad5f-416d-ab0c-a63893359177"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5163 - accuracy: 0.8167\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5163147449493408, 0.8166999816894531]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdUMo8Lodszl",
        "outputId": "849c0e2a-16eb-4e9c-e52a-42990b05f5fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "[1.3232124e-06 3.1351497e-08 6.8670702e-06 3.6073729e-06 1.2931750e-05\n",
            " 1.3436629e-01 2.4797379e-05 3.2358482e-01 6.4774537e-03 5.3552181e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkDDKZW4d4Dm",
        "outputId": "76927b63-b7a8-4a7e-c3b5-03f96a8da090"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "#training_images=training_images/255.0\n",
        "#test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVryPEDIg3RC",
        "outputId": "f66b5b2e-9f31-4a4e-9504-855bc60709a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 3.7826\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.5275\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.5051\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.4942\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4918\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.5256\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "[5.9557388e-22 1.6787556e-19 2.8280245e-28 2.8981205e-14 1.3866013e-28\n",
            " 2.3040401e-02 1.3857083e-27 8.5207343e-02 1.4519908e-16 8.9175224e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.88):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6b7B0rAhbQ7",
        "outputId": "fa7242d2-6864-4797-ee24-b84ef8712646"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4706 - accuracy: 0.8308\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3556 - accuracy: 0.8686\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8814\n",
            "Reached 95% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3221 - accuracy: 0.8814\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc88669b1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}